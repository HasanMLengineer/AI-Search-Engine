{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e43facdf-013f-4235-bbe7-466f37124fad",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78b53ee8-9e17-4018-82e2-3c8f2741d065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T12:02:43.057135Z",
     "iopub.status.busy": "2024-10-28T12:02:43.056775Z",
     "iopub.status.idle": "2024-10-28T12:02:44.424635Z",
     "shell.execute_reply": "2024-10-28T12:02:44.424216Z",
     "shell.execute_reply.started": "2024-10-28T12:02:43.057109Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "from llama_cpp import Llama\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d24aea4c-af1c-4e05-ac67-0c5b04db1c29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T12:02:54.704302Z",
     "iopub.status.busy": "2024-10-28T12:02:54.704074Z",
     "iopub.status.idle": "2024-10-28T12:02:54.706785Z",
     "shell.execute_reply": "2024-10-28T12:02:54.706369Z",
     "shell.execute_reply.started": "2024-10-28T12:02:54.704285Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load collection descriptions from JSON file\n",
    "def load_collection_descriptions(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a245df1-a095-4f59-9bdf-417bdaba9ad2",
   "metadata": {},
   "source": [
    "# Mongodb Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fcc4983-9f65-4fe5-a016-8e847002af08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T12:02:46.038139Z",
     "iopub.status.busy": "2024-10-28T12:02:46.037909Z",
     "iopub.status.idle": "2024-10-28T12:02:46.040963Z",
     "shell.execute_reply": "2024-10-28T12:02:46.040625Z",
     "shell.execute_reply.started": "2024-10-28T12:02:46.038123Z"
    }
   },
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "def connect_to_mongodb(db_uri, db_name):\n",
    "    client = MongoClient(db_uri, server_api=ServerApi('1'))\n",
    "    try:\n",
    "        client.admin.command('ping')\n",
    "        print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not connect to MongoDB:\", e)\n",
    "        return None\n",
    "    db = client[db_name]\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a08922f-8846-460a-b214-a681951cb671",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T12:02:56.523843Z",
     "iopub.status.busy": "2024-10-28T12:02:56.523554Z",
     "iopub.status.idle": "2024-10-28T12:02:57.207320Z",
     "shell.execute_reply": "2024-10-28T12:02:57.206606Z",
     "shell.execute_reply.started": "2024-10-28T12:02:56.523823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "MONGODB_LINK = \"mongodb_link\"\n",
    "DB_NAME = \"woice-search-engine\"\n",
    "db = connect_to_mongodb(MONGODB_LINK, DB_NAME)\n",
    "collection_descriptions = load_collection_descriptions(\"collections.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a78c740-15b5-4f4a-9e39-8e5fc5f00fcc",
   "metadata": {},
   "source": [
    "# Llama 3.1 GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6434ef8a-dda3-43bf-b5ba-347095d6255d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T10:49:35.038054Z",
     "iopub.status.busy": "2024-10-28T10:49:35.037731Z",
     "iopub.status.idle": "2024-10-28T10:49:36.520776Z",
     "shell.execute_reply": "2024-10-28T10:49:36.520427Z",
     "shell.execute_reply.started": "2024-10-28T10:49:35.038029Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 27 key-value pairs and 292 tensors from /mnt/SSD/models/Llama-3.1-8B-GGUF/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Hermes 3 Llama 3.1 8B\n",
      "llama_model_loader: - kv   3:                       general.organization str              = NousResearch\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Hermes-3-Llama-3.1\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   7:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   8:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  11:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  13:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128040\n",
      "llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128040\n",
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7994 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = Hermes 3 Llama 3.1 8B\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128040 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 128040 '<|im_end|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128040 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   281.81 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4403.50 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128040', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '15', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.padding_token_id': '128040', 'general.basename': 'Hermes-3-Llama-3.1', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Hermes 3 Llama 3.1 8B', 'general.organization': 'NousResearch', 'general.type': 'model', 'general.size_label': '8B', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# Load the GGUF model\n",
    "model_name_or_path = '/mnt/SSD/models/Llama-3.1-8B-GGUF/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf'\n",
    "model = Llama(\n",
    "    model_path=model_name_or_path,\n",
    "    n_ctx=8192,\n",
    "    n_gpu_layers=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5809472-3d0d-404b-80bb-bfb81bdefa54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T09:39:41.577466Z",
     "iopub.status.busy": "2024-10-23T09:39:41.577247Z",
     "iopub.status.idle": "2024-10-23T09:39:43.122844Z",
     "shell.execute_reply": "2024-10-23T09:39:43.122484Z",
     "shell.execute_reply.started": "2024-10-23T09:39:41.577450Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /mnt/SSD/models/Llama-3-8B-instruct-GGUF/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   281.81 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4403.49 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 80000\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 32\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size = 10000.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 10000.00 MiB, K (f16): 5000.00 MiB, V (f16): 5000.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   324.27 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    10.27 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\", 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'llama.context_length': '8192', 'general.name': '.', 'llama.vocab_size': '128256', 'general.file_type': '15', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# # Load the GGUF model\n",
    "# model_name_or_path = '/mnt/SSD/models/Llama-3-8B-instruct-GGUF/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf'\n",
    "# model = Llama(\n",
    "#     model_path=model_name_or_path,\n",
    "#     n_ctx=80000,\n",
    "#     n_batch=1,\n",
    "#     n_gpu_layers=-1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75de422a-5694-42b1-8316-783fd004adc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T10:49:44.951401Z",
     "iopub.status.busy": "2024-10-28T10:49:44.951036Z",
     "iopub.status.idle": "2024-10-28T10:49:44.954738Z",
     "shell.execute_reply": "2024-10-28T10:49:44.954190Z",
     "shell.execute_reply.started": "2024-10-28T10:49:44.951383Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_response(messages, model, temperature=0.8, top_p=0.8, top_k=20, max_tokens=150):\n",
    "    outputs = model.create_chat_completion(\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k\n",
    "    )\n",
    "    return outputs[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b33d55-7344-47dd-bdd9-41f569707584",
   "metadata": {},
   "source": [
    "# Query Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74080cad-b144-4f38-8274-72d97f8b040e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T10:49:50.630375Z",
     "iopub.status.busy": "2024-10-28T10:49:50.630056Z",
     "iopub.status.idle": "2024-10-28T10:49:50.637661Z",
     "shell.execute_reply": "2024-10-28T10:49:50.636965Z",
     "shell.execute_reply.started": "2024-10-28T10:49:50.630349Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_classification(query):\n",
    "    # Define the messages and function schema for the model\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"\"\"You are an ecommerce website where the user will search. You are tasked with classifying a user query. Based on the intent of the query, you will match\n",
    "    it to one of the available collections provided in the list. If the query matches more than one collection, show all of them in the output. Please always return the name of the matching collection\n",
    "    in valid JSON format: {\"collections\": [\"collection_name1\", \"collection_name2\", ...]}.\"\"\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": f\"The available collections are:\\n{collection_descriptions}\\nThe user query is: {query}\"}\n",
    "    ]\n",
    "\n",
    "    # Define the function to be called\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"classify_query\",\n",
    "                \"description\": \"Classify the user's query to matching collections.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"collections\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"string\"\n",
    "                            },\n",
    "                            \"description\": \"List of collections matching the user's query.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"collections\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Generate the output using the model with function calling\n",
    "    response = model.create_chat_completion(\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "    )\n",
    "\n",
    "    response_message = response[\"choices\"][0][\"message\"]\n",
    "    \n",
    "    # Check if the model intended to call a function\n",
    "    tool_calls = response_message.get(\"tool_calls\")\n",
    "    if tool_calls:\n",
    "        # Process function calls\n",
    "        for tool_call in tool_calls:\n",
    "            if tool_call.function.name == \"classify_query\":\n",
    "                # Simulating the function call for demonstration\n",
    "                # In a real scenario, implement your classification logic here\n",
    "                matched_collections = [\"example_collection_1\", \"example_collection_2\"]  # Mocked response\n",
    "                return json.dumps({\"collections\": matched_collections})\n",
    "\n",
    "    return response_message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8811a23b-2088-428b-ad27-05e74d15b5bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T10:49:54.221701Z",
     "iopub.status.busy": "2024-10-28T10:49:54.221351Z",
     "iopub.status.idle": "2024-10-28T10:49:57.074030Z",
     "shell.execute_reply": "2024-10-28T10:49:57.073637Z",
     "shell.execute_reply.started": "2024-10-28T10:49:54.221675Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     175.75 ms\n",
      "llama_print_timings:      sample time =       1.23 ms /    19 runs   (    0.06 ms per token, 15484.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2596.73 ms /  7264 tokens (    0.36 ms per token,  2797.37 tokens per second)\n",
      "llama_print_timings:        eval time =     205.30 ms /    18 runs   (   11.41 ms per token,    87.67 tokens per second)\n",
      "llama_print_timings:       total time =    2836.27 ms /  7282 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"collections\": [\"Kitchen and Home Appliances\", \"All Appliances\", \"Home Entertainment Systems\"]}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"I want to buy any appliance that keeps my fruits fresh in hot weather\"\n",
    "filtered_collection = query_classification(query=query)\n",
    "\n",
    "# Output the filtered collections\n",
    "print(filtered_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02f37a99-1bc8-4729-a408-ef103348632f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T11:32:06.157375Z",
     "iopub.status.busy": "2024-10-23T11:32:06.157165Z",
     "iopub.status.idle": "2024-10-23T11:32:07.074929Z",
     "shell.execute_reply": "2024-10-23T11:32:07.074581Z",
     "shell.execute_reply.started": "2024-10-23T11:32:06.157359Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 7249 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =     254.78 ms\n",
      "llama_print_timings:      sample time =       3.33 ms /    52 runs   (    0.06 ms per token, 15634.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =      47.31 ms /    10 tokens (    4.73 ms per token,   211.35 tokens per second)\n",
      "llama_print_timings:        eval time =     828.91 ms /    51 runs   (   16.25 ms per token,    61.53 tokens per second)\n",
      "llama_print_timings:       total time =     907.96 ms /    61 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"collections\": [\"Baby Products\", \"Nursing and Feeding\", \"Baby Bath Skin and Grooming\", \"Baby Fashion\", \"Toys and Games\", \"STEM Toys Store\", \"Toys Gifting Store\", \"International Toy Store\"]}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"I want to buy a gift for a baby\"\n",
    "filtered_collection = query_classification(query=query)\n",
    "\n",
    "# Output the filtered collections\n",
    "print(filtered_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d4b01b4-2780-4152-857e-ddae1d03366a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T10:35:35.069835Z",
     "iopub.status.busy": "2024-10-22T10:35:35.069644Z",
     "iopub.status.idle": "2024-10-22T10:35:35.524121Z",
     "shell.execute_reply": "2024-10-22T10:35:35.523766Z",
     "shell.execute_reply.started": "2024-10-22T10:35:35.069821Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 7248 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =     212.74 ms\n",
      "llama_print_timings:      sample time =       2.28 ms /    36 runs   (    0.06 ms per token, 15817.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =      32.82 ms /    10 tokens (    3.28 ms per token,   304.72 tokens per second)\n",
      "llama_print_timings:        eval time =     393.65 ms /    35 runs   (   11.25 ms per token,    88.91 tokens per second)\n",
      "llama_print_timings:       total time =     444.12 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"collections\": [\"Amazon Fashion\", \"Womens Fashion\", \"Mens Fashion\", \"Kids Fashion\", \"Ethnic Wear\", \"Fashion Sandals\", \"Shirts\"]}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"I want to do some shopping for Eid\"\n",
    "filtered_collection = query_classification(query=query)\n",
    "\n",
    "# Output the filtered collections\n",
    "print(filtered_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d96c2a8-a00e-4cf9-bbb1-3851c1210894",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T12:26:17.061326Z",
     "iopub.status.busy": "2024-10-22T12:26:17.061006Z",
     "iopub.status.idle": "2024-10-22T12:26:17.306294Z",
     "shell.execute_reply": "2024-10-22T12:26:17.305892Z",
     "shell.execute_reply.started": "2024-10-22T12:26:17.061301Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 7248 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =     212.74 ms\n",
      "llama_print_timings:      sample time =       1.13 ms /    17 runs   (    0.07 ms per token, 15004.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =      36.94 ms /    13 tokens (    2.84 ms per token,   351.97 tokens per second)\n",
      "llama_print_timings:        eval time =     184.41 ms /    16 runs   (   11.53 ms per token,    86.76 tokens per second)\n",
      "llama_print_timings:       total time =     231.02 ms /    29 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"collections\": [\"All Grocery and Gourmet Foods\", \"Snack Foods\"]}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"I want to do something that i can eat quickly.\"\n",
    "filtered_collection = query_classification(query=query)\n",
    "\n",
    "# Output the filtered collections\n",
    "print(filtered_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2ce9100-d646-44fd-b107-ccadc61040fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T11:24:52.691358Z",
     "iopub.status.busy": "2024-10-22T11:24:52.690992Z",
     "iopub.status.idle": "2024-10-22T11:24:52.812212Z",
     "shell.execute_reply": "2024-10-22T11:24:52.811914Z",
     "shell.execute_reply.started": "2024-10-22T11:24:52.691327Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 7248 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =     212.74 ms\n",
      "llama_print_timings:      sample time =       0.45 ms /     7 runs   (    0.06 ms per token, 15695.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =      36.38 ms /    13 tokens (    2.80 ms per token,   357.37 tokens per second)\n",
      "llama_print_timings:        eval time =      70.97 ms /     6 runs   (   11.83 ms per token,    84.54 tokens per second)\n",
      "llama_print_timings:       total time =     111.53 ms /    19 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"collections\": [\"Football\"]}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"I want to buy gift for a person who plays football\"\n",
    "filtered_collection = query_classification(query=query)\n",
    "\n",
    "# Output the filtered collections\n",
    "print(filtered_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a2554d",
   "metadata": {},
   "source": [
    "# Displaying Output of Product from Mongodb Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06f8de41-d10c-43af-8c1e-78feecb738b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-24T06:40:06.532826Z",
     "iopub.status.busy": "2024-10-24T06:40:06.532236Z",
     "iopub.status.idle": "2024-10-24T06:40:06.539693Z",
     "shell.execute_reply": "2024-10-24T06:40:06.539006Z",
     "shell.execute_reply.started": "2024-10-24T06:40:06.532786Z"
    }
   },
   "outputs": [],
   "source": [
    "def identify_keywords(query):\n",
    "    # Define the messages and function schema for the model\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"\"\"You are an advanced e-commerce search assistant. Your primary responsibility is to analyze the user's query and deduce their underlying intent, even when they do not specify product names directly. \n",
    "        Your response should focus on extracting essential concepts related to the query based on the provided products list. Always ensure your output is in valid JSON format: {\"Products\": [\"Product_name1\", \"Product_name2\", ...]} and make it as comprehensive and relevant as possible to the user's intent. Limit your suggestions to a maximum of 20 unique products to ensure the response is concise and relevant.\"\"\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": f\"The available collections are:\\n{Product_Names}\\nThe user query is: {query}\"}\n",
    "    ]\n",
    "\n",
    "    # Define the function to be called\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"suggest_keywords\",\n",
    "                \"description\": \"Understand the user's query to show him relevant products.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"collections\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"string\"\n",
    "                            },\n",
    "                            \"description\": \"List of products matching the user's query.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"products\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Generate the output using the model with function calling\n",
    "    response = model.create_chat_completion(\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "    )\n",
    "\n",
    "    response_message = response[\"choices\"][0][\"message\"]\n",
    "    \n",
    "    # Check if the model intended to call a function\n",
    "    tool_calls = response_message.get(\"tool_calls\")\n",
    "    if tool_calls:\n",
    "        # Process function calls\n",
    "        for tool_call in tool_calls:\n",
    "            if tool_call.function.name == \"classify_query\":\n",
    "                # Simulating the function call for demonstration\n",
    "                # In a real scenario, implement your classification logic here\n",
    "                matched_collections = [\"example_collection_1\", \"example_collection_2\"]  # Mocked response\n",
    "                return json.dumps({\"collections\": matched_collections})\n",
    "\n",
    "    return response_message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c81420a8-e536-4c75-90cb-8fa381dbfee9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T07:54:50.989559Z",
     "iopub.status.busy": "2024-10-23T07:54:50.989221Z",
     "iopub.status.idle": "2024-10-23T07:55:15.390766Z",
     "shell.execute_reply": "2024-10-23T07:55:15.390424Z",
     "shell.execute_reply.started": "2024-10-23T07:54:50.989538Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     170.38 ms\n",
      "llama_print_timings:      sample time =      23.24 ms /   362 runs   (    0.06 ms per token, 15577.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16846.05 ms / 23687 tokens (    0.71 ms per token,  1406.09 tokens per second)\n",
      "llama_print_timings:        eval time =    7130.14 ms /   361 runs   (   19.75 ms per token,    50.63 tokens per second)\n",
      "llama_print_timings:       total time =   24368.63 ms / 24048 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Products\": [\"LG 1.5 Ton 3 Star AI DUAL Inverter Split AC (Copper, Super Convertible 6-in-1 Cooling, HD Filter with Anti-Virus Protection, 2023 Model, White, Gls18I3...\", \"Voltas 1.5 Ton 3 Star Inverter Split AC (Copper 183V CZQ White)\", \"Daikin 1.5 Ton 3 Star Inverter Split AC (Copper DTKL50, White)\", \"Carrier 1.5 Ton 3 Star Hybridjet Inverter Split AC CAI18IN5R31W1 (Copper, INDUS CXI, 6-in-1 Flexicool with Anti-Viral Guard, Smart Energy, 2023 Model, White)\", \"Blue Star 1.5 Ton 3 Star Inverter Split Ac (Copper,IA318YKU, 2022, White)\", \"Samsung 1.5 Ton 3 Star Windfree Technology Inverter Split AC (Copper, Convertible 5-in-1 Cooling Mode, Tri Care Filter, 2023 Model AR18...\", \"Hitachi 1.5 Ton 3 Star Split AC (Copper RSNG318HEDO white)\", \"Godrej 1.5 Ton 3 Star Inverter Split AC (Copper GIC 18WTC3-WSB White)\", \"Whirlpool 1.5 Ton 3 Star, Flexicool Inverter Split AC (Copper, Convertible 4-in-1 Cooling Mode, HD Filter 2023 Model, S3K2...\", \"LG 1.5 Ton 3 Star Inverter Split AC (Copper KS-Q18ENXA White)\"]}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"I want to buy any appliance that keeps my fruits fresh in hot weather\"\n",
    "filtered_collection = identify_keywords(query=query)\n",
    "\n",
    "# Output the filtered collections\n",
    "print(filtered_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36127d3f-e8d1-46ed-afc7-f09e7e5bf033",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T05:59:35.813930Z",
     "iopub.status.busy": "2024-10-23T05:59:35.813607Z",
     "iopub.status.idle": "2024-10-23T05:59:36.664423Z",
     "shell.execute_reply": "2024-10-23T05:59:36.664010Z",
     "shell.execute_reply.started": "2024-10-23T05:59:35.813903Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 6 prefix-match hit, remaining 155 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =      84.94 ms\n",
      "llama_print_timings:      sample time =       6.11 ms /    96 runs   (    0.06 ms per token, 15706.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =      55.29 ms /   155 tokens (    0.36 ms per token,  2803.55 tokens per second)\n",
      "llama_print_timings:        eval time =     748.50 ms /    95 runs   (    7.88 ms per token,   126.92 tokens per second)\n",
      "llama_print_timings:       total time =     844.55 ms /   250 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Products\": [\"Fridge\", \"Freezer\", \"Refrigerator\", \"Chest Freezer\", \"Fruit Basket\", \"Fruit Cooler\", \"Fridge Basket\", \"Portable Fridge\", \"Insulated Fridge\", \"Fruit Preservation Appliance\", \"Fruit Storage Box\", \"Fruit Humidifier\", \"Fruit Freshness Kit\", \"Fruit Preservation System\", \"Fridge for Fruits\", \"Fruit Preservation Container\"]}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"I want to buy any appliance that keeps my fruits fresh in hot weather\"\n",
    "filtered_collection = identify_keywords(query=query)\n",
    "\n",
    "# Output the filtered collections\n",
    "print(filtered_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e08a4b87-7c35-40fd-b6c1-33253cd6b4d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T06:14:14.210642Z",
     "iopub.status.busy": "2024-10-23T06:14:14.209996Z",
     "iopub.status.idle": "2024-10-23T06:14:15.151364Z",
     "shell.execute_reply": "2024-10-23T06:14:15.151004Z",
     "shell.execute_reply.started": "2024-10-23T06:14:14.210617Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 132 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =      84.94 ms\n",
      "llama_print_timings:      sample time =       6.87 ms /   107 runs   (    0.06 ms per token, 15568.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =      45.51 ms /    44 tokens (    1.03 ms per token,   966.80 tokens per second)\n",
      "llama_print_timings:        eval time =     844.86 ms /   106 runs   (    7.97 ms per token,   125.46 tokens per second)\n",
      "llama_print_timings:       total time =     934.76 ms /   150 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the JSON response with relevant product suggestions for a baby gift:\n",
      "\n",
      "{\"Products\": [\"Baby onesie\", \"Baby blanket\", \"Baby bib\", \"Baby rattle\", \"Baby mobile\", \"Baby bath tub\", \"Baby carrier\", \"Baby stroller\", \"Baby monitor\", \"Baby toy\", \"Baby book\", \"Baby shoes\", \"Baby socks\", \"Baby hat\", \"Baby mittens\", \"Baby swaddle\", \"Baby crib\", \"Baby changing table\", \"Baby safety gate\", \"Baby nursery set\"]}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"I want to buy a gift for a baby\"\n",
    "filtered_collection = identify_keywords(query=query)\n",
    "\n",
    "# Output the filtered collections\n",
    "print(filtered_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96275999-132a-413d-b4cb-c38b428b7816",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T06:22:09.553419Z",
     "iopub.status.busy": "2024-10-23T06:22:09.553185Z",
     "iopub.status.idle": "2024-10-23T06:22:10.802782Z",
     "shell.execute_reply": "2024-10-23T06:22:10.802445Z",
     "shell.execute_reply.started": "2024-10-23T06:22:09.553405Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 167 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =      84.94 ms\n",
      "llama_print_timings:      sample time =       9.24 ms /   144 runs   (    0.06 ms per token, 15591.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =      40.09 ms /     9 tokens (    4.45 ms per token,   224.50 tokens per second)\n",
      "llama_print_timings:        eval time =    1141.77 ms /   143 runs   (    7.98 ms per token,   125.24 tokens per second)\n",
      "llama_print_timings:       total time =    1244.71 ms /   152 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the user's query, it seems they are looking for food items that pair well with tea. Here is a list of relevant products that could be suggested:\n",
      "\n",
      "{\"Products\": [\"Tea biscuits\", \"Scones\", \"Shortbread cookies\", \"Fruit pastries\", \"Cheese straws\", \"Chocolate truffles\", \"Caramel corn\", \"Mixed nuts\", \"Assorted chocolates\", \"Tea sandwiches\", \"Assorted crackers\", \"Cheese and fruit platter\", \"Assorted tea cakes\", \"Tea loaf\", \"Tea cookies\", \"Tea breads\", \"Tea biscuits\", \"Tea cakes\", \"Tea sandwiches\", \"Assorted pastries\"]}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"I want to buy something to eat with tea\"\n",
    "filtered_collection = identify_keywords(query=query)\n",
    "\n",
    "# Output the filtered collections\n",
    "print(filtered_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8de47c4-c8d1-4ab6-bf3d-3a64c4f7b8d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T10:50:27.520142Z",
     "iopub.status.busy": "2024-10-28T10:50:27.519947Z",
     "iopub.status.idle": "2024-10-28T10:50:27.522987Z",
     "shell.execute_reply": "2024-10-28T10:50:27.522637Z",
     "shell.execute_reply.started": "2024-10-28T10:50:27.520128Z"
    }
   },
   "outputs": [],
   "source": [
    "# def generate_response(name, main_category, sub_category):\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": \"You are an ecommerce assistant. Generate a product description based on the given details. The description of product should include all the keywords relevant to the product.\"},\n",
    "#         {\"role\": \"user\", \"content\": f\"Name: {name}\\nMain Category: {main_category}\\nSub Category: {sub_category}\"}\n",
    "#     ]\n",
    "    \n",
    "#     response = model.create_chat_completion(\n",
    "#         messages=messages,\n",
    "#         max_tokens=150,\n",
    "#         temperature=0.8,\n",
    "#         top_p=0.8,\n",
    "#         top_k=20\n",
    "#     )\n",
    "    \n",
    "#     return response[\"choices\"][0][\"message\"][\"content\"].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc045aa4-7479-4892-a3f3-158df8ab62e5",
   "metadata": {},
   "source": [
    "# Upload Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcfe088e-641f-4ebd-8e08-7dc6ce0b72d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T06:07:24.067716Z",
     "iopub.status.busy": "2024-10-28T06:07:24.067520Z",
     "iopub.status.idle": "2024-10-28T06:07:24.070562Z",
     "shell.execute_reply": "2024-10-28T06:07:24.070162Z",
     "shell.execute_reply.started": "2024-10-28T06:07:24.067703Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_response(name, main_category, sub_category):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an ecommerce assistant. Generate a product description based on the given details. Include product's name, all relevant keywords, features and use cases of product.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Name: {name}\\nMain Category: {main_category}\\nSub Category: {sub_category}\"}\n",
    "    ]\n",
    "    \n",
    "    response = model.create_chat_completion(\n",
    "        messages=messages,\n",
    "        max_tokens=500,\n",
    "        temperature=0.8,\n",
    "        top_p=0.8,\n",
    "        top_k=20\n",
    "    )\n",
    "    \n",
    "    return response[\"choices\"][0][\"message\"][\"content\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a5b388a-c8d9-4a7b-a344-55fc66d1544d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T06:07:26.102389Z",
     "iopub.status.busy": "2024-10-28T06:07:26.102130Z",
     "iopub.status.idle": "2024-10-28T06:08:32.288516Z",
     "shell.execute_reply": "2024-10-28T06:08:32.288133Z",
     "shell.execute_reply.started": "2024-10-28T06:07:26.102369Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 36 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      18.41 ms /   305 runs   (    0.06 ms per token, 16567.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =      33.19 ms /    36 tokens (    0.92 ms per token,  1084.73 tokens per second)\n",
      "llama_print_timings:        eval time =    2414.05 ms /   304 runs   (    7.94 ms per token,   125.93 tokens per second)\n",
      "llama_print_timings:       total time =    2605.40 ms /   340 tokens\n",
      "Llama.generate: 44 prefix-match hit, remaining 25 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c6408944 with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      17.35 ms /   288 runs   (    0.06 ms per token, 16601.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =      14.29 ms /    25 tokens (    0.57 ms per token,  1749.72 tokens per second)\n",
      "llama_print_timings:        eval time =    2271.51 ms /   287 runs   (    7.91 ms per token,   126.35 tokens per second)\n",
      "llama_print_timings:       total time =    2432.41 ms /   312 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 29 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c6408945 with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      14.02 ms /   234 runs   (    0.06 ms per token, 16685.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =      14.35 ms /    29 tokens (    0.49 ms per token,  2020.48 tokens per second)\n",
      "llama_print_timings:        eval time =    1838.00 ms /   233 runs   (    7.89 ms per token,   126.77 tokens per second)\n",
      "llama_print_timings:       total time =    1964.87 ms /   262 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 29 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c6408946 with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      19.72 ms /   327 runs   (    0.06 ms per token, 16583.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =      14.49 ms /    29 tokens (    0.50 ms per token,  2000.83 tokens per second)\n",
      "llama_print_timings:        eval time =    2587.66 ms /   326 runs   (    7.94 ms per token,   125.98 tokens per second)\n",
      "llama_print_timings:       total time =    2777.69 ms /   355 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 25 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c6408947 with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      17.40 ms /   284 runs   (    0.06 ms per token, 16321.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =      14.23 ms /    25 tokens (    0.57 ms per token,  1757.22 tokens per second)\n",
      "llama_print_timings:        eval time =    2245.12 ms /   283 runs   (    7.93 ms per token,   126.05 tokens per second)\n",
      "llama_print_timings:       total time =    2407.13 ms /   308 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 53 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c6408948 with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      25.91 ms /   402 runs   (    0.06 ms per token, 15517.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =      18.79 ms /    53 tokens (    0.35 ms per token,  2820.95 tokens per second)\n",
      "llama_print_timings:        eval time =    3225.58 ms /   401 runs   (    8.04 ms per token,   124.32 tokens per second)\n",
      "llama_print_timings:       total time =    3487.61 ms /   454 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 32 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c6408949 with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      17.77 ms /   269 runs   (    0.07 ms per token, 15138.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =      14.47 ms /    32 tokens (    0.45 ms per token,  2211.47 tokens per second)\n",
      "llama_print_timings:        eval time =    2138.25 ms /   268 runs   (    7.98 ms per token,   125.34 tokens per second)\n",
      "llama_print_timings:       total time =    2297.90 ms /   300 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 30 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c640894a with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      19.24 ms /   303 runs   (    0.06 ms per token, 15746.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =      14.77 ms /    30 tokens (    0.49 ms per token,  2030.73 tokens per second)\n",
      "llama_print_timings:        eval time =    2417.02 ms /   302 runs   (    8.00 ms per token,   124.95 tokens per second)\n",
      "llama_print_timings:       total time =    2597.43 ms /   332 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 36 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c640894b with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      18.85 ms /   310 runs   (    0.06 ms per token, 16446.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =      15.75 ms /    36 tokens (    0.44 ms per token,  2285.86 tokens per second)\n",
      "llama_print_timings:        eval time =    2460.47 ms /   309 runs   (    7.96 ms per token,   125.59 tokens per second)\n",
      "llama_print_timings:       total time =    2639.48 ms /   345 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 42 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c640894c with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      18.96 ms /   313 runs   (    0.06 ms per token, 16505.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =      16.53 ms /    42 tokens (    0.39 ms per token,  2541.14 tokens per second)\n",
      "llama_print_timings:        eval time =    2489.72 ms /   312 runs   (    7.98 ms per token,   125.32 tokens per second)\n",
      "llama_print_timings:       total time =    2671.74 ms /   354 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 32 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c640894d with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      16.66 ms /   275 runs   (    0.06 ms per token, 16507.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =      14.60 ms /    32 tokens (    0.46 ms per token,  2192.38 tokens per second)\n",
      "llama_print_timings:        eval time =    2178.78 ms /   274 runs   (    7.95 ms per token,   125.76 tokens per second)\n",
      "llama_print_timings:       total time =    2331.17 ms /   306 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 38 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c640894e with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      17.33 ms /   286 runs   (    0.06 ms per token, 16504.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =      15.75 ms /    38 tokens (    0.41 ms per token,  2412.85 tokens per second)\n",
      "llama_print_timings:        eval time =    2267.89 ms /   285 runs   (    7.96 ms per token,   125.67 tokens per second)\n",
      "llama_print_timings:       total time =    2428.59 ms /   323 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 43 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c640894f with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      14.52 ms /   242 runs   (    0.06 ms per token, 16662.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =      16.54 ms /    43 tokens (    0.38 ms per token,  2599.29 tokens per second)\n",
      "llama_print_timings:        eval time =    1911.10 ms /   241 runs   (    7.93 ms per token,   126.11 tokens per second)\n",
      "llama_print_timings:       total time =    2045.49 ms /   284 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 34 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c6408950 with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      20.75 ms /   343 runs   (    0.06 ms per token, 16528.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =      15.73 ms /    34 tokens (    0.46 ms per token,  2162.02 tokens per second)\n",
      "llama_print_timings:        eval time =    2733.21 ms /   342 runs   (    7.99 ms per token,   125.13 tokens per second)\n",
      "llama_print_timings:       total time =    2937.06 ms /   376 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 42 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c6408951 with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      18.02 ms /   300 runs   (    0.06 ms per token, 16652.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =      16.54 ms /    42 tokens (    0.39 ms per token,  2539.91 tokens per second)\n",
      "llama_print_timings:        eval time =    2381.79 ms /   299 runs   (    7.97 ms per token,   125.54 tokens per second)\n",
      "llama_print_timings:       total time =    2551.72 ms /   341 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 28 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c6408952 with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      17.45 ms /   289 runs   (    0.06 ms per token, 16560.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =      14.38 ms /    28 tokens (    0.51 ms per token,  1947.56 tokens per second)\n",
      "llama_print_timings:        eval time =    2295.69 ms /   288 runs   (    7.97 ms per token,   125.45 tokens per second)\n",
      "llama_print_timings:       total time =    2458.45 ms /   316 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 35 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c6408953 with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      20.90 ms /   342 runs   (    0.06 ms per token, 16362.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =      15.86 ms /    35 tokens (    0.45 ms per token,  2206.39 tokens per second)\n",
      "llama_print_timings:        eval time =    2737.20 ms /   341 runs   (    8.03 ms per token,   124.58 tokens per second)\n",
      "llama_print_timings:       total time =    2940.73 ms /   376 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 34 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c6408954 with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      25.36 ms /   419 runs   (    0.06 ms per token, 16525.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =      15.78 ms /    34 tokens (    0.46 ms per token,  2155.31 tokens per second)\n",
      "llama_print_timings:        eval time =    3364.95 ms /   418 runs   (    8.05 ms per token,   124.22 tokens per second)\n",
      "llama_print_timings:       total time =    3627.40 ms /   452 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 33 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c6408955 with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      19.74 ms /   328 runs   (    0.06 ms per token, 16616.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =      15.74 ms /    33 tokens (    0.48 ms per token,  2096.84 tokens per second)\n",
      "llama_print_timings:        eval time =    2612.66 ms /   327 runs   (    7.99 ms per token,   125.16 tokens per second)\n",
      "llama_print_timings:       total time =    2801.82 ms /   360 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 35 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c6408956 with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      20.26 ms /   336 runs   (    0.06 ms per token, 16582.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =      15.79 ms /    35 tokens (    0.45 ms per token,  2217.15 tokens per second)\n",
      "llama_print_timings:        eval time =    2671.98 ms /   335 runs   (    7.98 ms per token,   125.38 tokens per second)\n",
      "llama_print_timings:       total time =    2865.70 ms /   370 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 29 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c6408957 with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      26.43 ms /   438 runs   (    0.06 ms per token, 16574.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =      14.65 ms /    29 tokens (    0.51 ms per token,  1979.39 tokens per second)\n",
      "llama_print_timings:        eval time =    3500.55 ms /   437 runs   (    8.01 ms per token,   124.84 tokens per second)\n",
      "llama_print_timings:       total time =    3771.41 ms /   466 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 28 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c6408958 with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      16.71 ms /   277 runs   (    0.06 ms per token, 16580.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =      14.40 ms /    28 tokens (    0.51 ms per token,  1944.04 tokens per second)\n",
      "llama_print_timings:        eval time =    2193.67 ms /   276 runs   (    7.95 ms per token,   125.82 tokens per second)\n",
      "llama_print_timings:       total time =    2347.38 ms /   304 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 31 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c6408959 with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      16.93 ms /   281 runs   (    0.06 ms per token, 16592.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =      14.58 ms /    31 tokens (    0.47 ms per token,  2126.93 tokens per second)\n",
      "llama_print_timings:        eval time =    2227.55 ms /   280 runs   (    7.96 ms per token,   125.70 tokens per second)\n",
      "llama_print_timings:       total time =    2384.81 ms /   311 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 39 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c640895a with new description.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     222.02 ms\n",
      "llama_print_timings:      sample time =      18.79 ms /   313 runs   (    0.06 ms per token, 16655.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =      15.95 ms /    39 tokens (    0.41 ms per token,  2445.45 tokens per second)\n",
      "llama_print_timings:        eval time =    2485.26 ms /   312 runs   (    7.97 ms per token,   125.54 tokens per second)\n",
      "llama_print_timings:       total time =    2664.41 ms /   351 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product 670f9cb974b51486c640895b with new description.\n"
     ]
    }
   ],
   "source": [
    "def update_products_with_descriptions():\n",
    "    products_collection = db['International Toy Store']  # Replace with your actual collection name\n",
    "    products = products_collection.find({})\n",
    "\n",
    "    for product in products:\n",
    "        name = product.get('name')\n",
    "        main_category = product.get('main_category')\n",
    "        sub_category = product.get('sub_category')\n",
    "        \n",
    "        description = generate_response(name, main_category, sub_category)\n",
    "        \n",
    "        # Update the product with the new description\n",
    "        products_collection.update_one(\n",
    "            {'_id': product['_id']},\n",
    "            {'$set': {'description': description}}\n",
    "        )\n",
    "        print(f\"Updated product {product['_id']} with new description.\")\n",
    "\n",
    "# Execute the update function\n",
    "update_products_with_descriptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f736918-d75c-4b94-b961-898c080c6667",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T09:19:16.185255Z",
     "iopub.status.busy": "2024-10-28T09:19:16.184993Z",
     "iopub.status.idle": "2024-10-28T09:19:16.187861Z",
     "shell.execute_reply": "2024-10-28T09:19:16.187359Z",
     "shell.execute_reply.started": "2024-10-28T09:19:16.185235Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install torch==2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeeefec1-8614-4292-b7dd-c54b8ad59df0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T09:19:05.878881Z",
     "iopub.status.busy": "2024-10-28T09:19:05.878620Z",
     "iopub.status.idle": "2024-10-28T09:19:05.881660Z",
     "shell.execute_reply": "2024-10-28T09:19:05.881270Z",
     "shell.execute_reply.started": "2024-10-28T09:19:05.878861Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install torch==2.0.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0471a22-84de-4535-af6c-1694a87e8b3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T09:19:09.319611Z",
     "iopub.status.busy": "2024-10-28T09:19:09.319335Z",
     "iopub.status.idle": "2024-10-28T09:19:09.322200Z",
     "shell.execute_reply": "2024-10-28T09:19:09.321709Z",
     "shell.execute_reply.started": "2024-10-28T09:19:09.319590Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f75085c-f7c5-4c16-9592-886501ab5ce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T09:18:59.096315Z",
     "iopub.status.busy": "2024-10-28T09:18:59.096115Z",
     "iopub.status.idle": "2024-10-28T09:18:59.098400Z",
     "shell.execute_reply": "2024-10-28T09:18:59.098079Z",
     "shell.execute_reply.started": "2024-10-28T09:18:59.096302Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03e3ce4-4715-4684-b680-7f0de7456fb8",
   "metadata": {},
   "source": [
    "# Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "948bca2a-59db-4a11-b9cc-99613a04c697",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T08:15:42.152501Z",
     "iopub.status.busy": "2024-10-28T08:15:42.152156Z",
     "iopub.status.idle": "2024-10-28T08:15:43.150972Z",
     "shell.execute_reply": "2024-10-28T08:15:43.150561Z",
     "shell.execute_reply.started": "2024-10-28T08:15:42.152474Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9620775a-5117-4ced-8422-84d582fae03d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T09:30:48.693424Z",
     "iopub.status.busy": "2024-10-28T09:30:48.693115Z",
     "iopub.status.idle": "2024-10-28T09:30:48.695650Z",
     "shell.execute_reply": "2024-10-28T09:30:48.695262Z",
     "shell.execute_reply.started": "2024-10-28T09:30:48.693406Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c88dd88-5f35-495b-af92-2cd7f94f4ecd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T12:03:10.945964Z",
     "iopub.status.busy": "2024-10-28T12:03:10.945642Z",
     "iopub.status.idle": "2024-10-28T12:03:10.949405Z",
     "shell.execute_reply": "2024-10-28T12:03:10.948851Z",
     "shell.execute_reply.started": "2024-10-28T12:03:10.945939Z"
    }
   },
   "outputs": [],
   "source": [
    "collection = db['International Toy Store'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb840dcf-d12c-4c14-b0fb-9f6f4b95fc1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T12:03:12.113376Z",
     "iopub.status.busy": "2024-10-28T12:03:12.113051Z",
     "iopub.status.idle": "2024-10-28T12:03:14.575716Z",
     "shell.execute_reply": "2024-10-28T12:03:14.575186Z",
     "shell.execute_reply.started": "2024-10-28T12:03:12.113350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model BAAI/bge-large-en-v1.5 on cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "modelID = 'BAAI/bge-large-en-v1.5'\n",
    "print(f\"Loading model {modelID} on {device}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelID, use_fast=True)\n",
    "model = AutoModel.from_pretrained(modelID).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5453f1e",
   "metadata": {},
   "source": [
    "## Generate Vector Embeddings using BGE Large Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f67000d4-da3c-47a1-8280-474f5ca7c9c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T12:03:15.261214Z",
     "iopub.status.busy": "2024-10-28T12:03:15.260684Z",
     "iopub.status.idle": "2024-10-28T12:03:15.265429Z",
     "shell.execute_reply": "2024-10-28T12:03:15.264905Z",
     "shell.execute_reply.started": "2024-10-28T12:03:15.261199Z"
    }
   },
   "outputs": [],
   "source": [
    "class VectorEmbeddingGenerator:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model.to(device)\n",
    "\n",
    "    def generate_embeddings(self, descriptions):\n",
    "        embeddings = []\n",
    "        for desc in descriptions:\n",
    "            emb = self._get_query_embedding(desc)\n",
    "            embeddings.append(emb)\n",
    "        return np.vstack(embeddings)\n",
    "\n",
    "    def _get_query_embedding(self, query):\n",
    "        inputs = self.tokenizer(query, padding=True, truncation=True, return_tensors='pt', max_length=256,\n",
    "                                add_special_tokens=True, return_attention_mask=True, return_token_type_ids=False)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs.to(device))\n",
    "\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        last_hidden = outputs.last_hidden_state.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "        embeddings = last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "        return embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f90c0f-9bd2-4821-9793-00060778df21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T09:34:51.260340Z",
     "iopub.status.busy": "2024-10-28T09:34:51.260066Z",
     "iopub.status.idle": "2024-10-28T09:34:53.876222Z",
     "shell.execute_reply": "2024-10-28T09:34:53.875494Z",
     "shell.execute_reply.started": "2024-10-28T09:34:51.260325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated and stored in MongoDB.\n"
     ]
    }
   ],
   "source": [
    "# Fetch descriptions from MongoDB\n",
    "descriptions = []\n",
    "documents = collection.find({}, {'description': 1})  # Fetch only the description field\n",
    "for doc in documents:\n",
    "    descriptions.append(doc['description'])\n",
    "\n",
    "# Generate embeddings for the descriptions\n",
    "embedding_generator = VectorEmbeddingGenerator(model, tokenizer)\n",
    "description_embeddings = embedding_generator.generate_embeddings(descriptions)\n",
    "\n",
    "# Optionally, store the embeddings back to MongoDB\n",
    "for i, doc in enumerate(collection.find()):\n",
    "    collection.update_one({'_id': doc['_id']}, {'$set': {'description_embedding': description_embeddings[i].tolist()}})\n",
    "\n",
    "print(\"Embeddings generated and stored in MongoDB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b920acd-6b66-43ba-b7cf-de7a4d022297",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T10:26:53.466863Z",
     "iopub.status.busy": "2024-11-04T10:26:53.466401Z",
     "iopub.status.idle": "2024-11-04T10:26:53.470103Z",
     "shell.execute_reply": "2024-11-04T10:26:53.469747Z",
     "shell.execute_reply.started": "2024-11-04T10:26:53.466838Z"
    }
   },
   "outputs": [],
   "source": [
    "def vector_search(collection, index='bgeVectors', path='description_embedding', num_candidates=100, limit=10, query=\"\"):\n",
    "    t = time()\n",
    "    logger.info(f\"Generating embedding for query: '{query}'\")\n",
    "    \n",
    "    # Get query embedding\n",
    "    query_vector = get_query_embedding(query=query).tolist()\n",
    "    logger.info(f\"Generated query vector: {query_vector[:5]}... (showing first 5 elements)\")\n",
    "    \n",
    "    # Construct MongoDB pipeline for vector search\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": index,\n",
    "                \"path\": path,\n",
    "                \"queryVector\": query_vector[0],\n",
    "                \"numCandidates\": num_candidates,\n",
    "                \"limit\": limit\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            '$project': {\n",
    "                '_id': 0, \n",
    "                'name': 1,\n",
    "                'main_category': 1,\n",
    "                'sub_category': 1,\n",
    "                'image': 1,\n",
    "                'link': 1,\n",
    "                'ratings': 1,\n",
    "                'no_of_ratings': 1,\n",
    "                'discount_price': 1,\n",
    "                'actual_price': 1,\n",
    "                'description': 1,\n",
    "                'score': {\"$meta\": \"vectorSearchScore\"}\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Log pipeline for debugging\n",
    "    logger.info(f\"Pipeline for MongoDB vector search: {pipeline}\")\n",
    "    \n",
    "    # Execute the pipeline\n",
    "    t = time()\n",
    "    results = collection.aggregate(pipeline)\n",
    "    logger.info(f'MongoDB vector search execution time: {time() - t:.3f} sec')\n",
    "\n",
    "    # Collect results and log number of results found\n",
    "    results_list = [res for res in results]\n",
    "    logger.info(f\"Number of results found: {len(results_list)}\")\n",
    "\n",
    "    if results_list:\n",
    "        for res in results_list:\n",
    "            logger.info(f\"Result: {res}\")\n",
    "    else:\n",
    "        logger.warning(\"No results found.\")\n",
    "\n",
    "    return results_list\n",
    "\n",
    "# Example usage\n",
    "query = \"3D puzzles for kids\"\n",
    "results = vector_search(collection=db['International Toy Store'], query=query)\n",
    "\n",
    "# Display results\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de025809-3481-4cc8-b599-2405c44e607f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T11:58:43.574773Z",
     "iopub.status.busy": "2024-10-28T11:58:43.574368Z",
     "iopub.status.idle": "2024-10-28T11:58:43.580014Z",
     "shell.execute_reply": "2024-10-28T11:58:43.579403Z",
     "shell.execute_reply.started": "2024-10-28T11:58:43.574718Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_query_embedding(query):\n",
    "    inputs = tokenizer(query, padding=True, truncation=True, return_tensors='pt', max_length=256,\n",
    "                       add_special_tokens=True, return_attention_mask=True, return_token_type_ids=False)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs.to(device))\n",
    "\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    last_hidden = outputs.last_hidden_state.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    embeddings = last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "    return embeddings.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33ec0c84-4790-49df-8f01-a0aeb85ee99a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T12:02:19.085845Z",
     "iopub.status.busy": "2024-10-28T12:02:19.085512Z",
     "iopub.status.idle": "2024-10-28T12:02:19.369421Z",
     "shell.execute_reply": "2024-10-28T12:02:19.369012Z",
     "shell.execute_reply.started": "2024-10-28T12:02:19.085818Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def semantic_search(query, top_n=5):\n",
    "    # Get the query embedding\n",
    "    query_embedding = get_query_embedding(query)\n",
    "\n",
    "    # Fetch all stored embeddings from MongoDB\n",
    "    stored_embeddings = []\n",
    "    ids = []\n",
    "    documents = collection.find({}, {'_id': 1, 'description_embedding': 1})  # Fetch IDs and embeddings\n",
    "    for doc in documents:\n",
    "        stored_embeddings.append(doc['description_embedding'])\n",
    "        ids.append(doc['_id'])\n",
    "\n",
    "    # Convert stored embeddings to numpy array\n",
    "    stored_embeddings = np.array(stored_embeddings)\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    similarities = cosine_similarity(query_embedding, stored_embeddings)\n",
    "\n",
    "    # Get the top_n results\n",
    "    top_indices = np.argsort(similarities[0])[::-1][:top_n]\n",
    "    top_results = [(ids[i], similarities[0][i]) for i in top_indices]\n",
    "\n",
    "    return top_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8329ba4b-edcc-4387-aeb2-f4c104fb1086",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T11:55:07.958247Z",
     "iopub.status.busy": "2024-10-28T11:55:07.957913Z",
     "iopub.status.idle": "2024-10-28T11:55:08.091664Z",
     "shell.execute_reply": "2024-10-28T11:55:08.090933Z",
     "shell.execute_reply.started": "2024-10-28T11:55:07.958220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document: {'_id': ObjectId('670f9cb974b51486c6408944'), 'name': 'Ravensburger 3D Puzzles Big Ben Night Edition, Multi Color (216 Pieces)', 'main_category': 'toys & baby products', 'sub_category': 'International Toy Store', 'image': 'https://m.media-amazon.com/images/W/IMAGERENDERING_521856-T1/images/I/61r8g1fjWTL._AC_UL320_.jpg', 'link': 'https://www.amazon.in/Ravensburger-Puzzles-Night-Multi-Pieces/dp/B00QM4W042/ref=sr_1_289?qid=1679219918&s=toys&sr=1-289', 'ratings': 4.6, 'no_of_ratings': '5,017', 'discount_price': nan, 'actual_price': '₹6,752', 'description': \"Introducing the Ravensburger 3D Puzzles Big Ben Night Edition, a spectacular addition to your collection of multi-colored, intricate and fascinating puzzles. With 216 high-quality pieces, this puzzle is perfect for both adults and children who love a challenge and a stunning end result.\\n\\nKey Features:\\n- Brand: Ravensburger, a renowned leader in jigsaw puzzles and quality toys\\n- Theme: Big Ben at night, featuring the iconic London landmark illuminated under a starry sky\\n- Multi-color design with 216 pieces for a challenging yet rewarding experience\\n- Made from thick, durable cardboard for easy handling and less breakage\\n- Interlocking pieces ensure smooth edges and precise fitting for a seamless finish\\n- Suitable for ages 6 and up, making it a perfect gift for puzzle enthusiasts of all ages\\n- Puzzle dimensions: 18.1 x 26.2 x 2.8 inches (packed)\\n\\nUse Cases:\\n- Perfect for family game nights or solo puzzling sessions\\n- Great for developing problem-solving and fine motor skills in children\\n- Makes for an ideal travel accessory, keeping you entertained during long journeys\\n- Ideal for those who love a good challenge and enjoy the satisfaction of completing a complex puzzle\\n- A wonderful addition to any home or office décor, showcasing the beauty of London's most famous landmark\\n\\nProduct Name: Ravensburger 3D Puzzles Big Ben Night Edition, Multi Color (216 Pieces)\\nMain Category: Toys & Baby Products\\nSub Category: International Toy Store\", 'description_embedding': [0.633315920829773, -0.2521599531173706, 0.26829957962036133, 0.18115323781967163, -0.5903775095939636, -0.04089760035276413, -0.29384326934814453, 0.734052300453186, -0.2378198355436325, 0.03432097285985947, 0.45781591534614563, 0.3902641236782074, -0.40192893147468567, -0.13289566338062286, -0.26624810695648193, 0.18288493156433105, -1.0204048156738281, -0.6212682127952576, -0.40741676092147827, -0.2596561312675476, 0.20176200568675995, 0.3193280100822449, -0.9007040858268738, -0.19947198033332825, -0.11087700724601746, 1.1805479526519775, 0.11646361649036407, 0.23222167789936066, 0.7006696462631226, 0.8467556834220886, -0.26263687014579773, -0.6155997514724731, 0.6871600151062012, -0.5287843942642212, -0.3931068181991577, 0.23334693908691406, 0.731537938117981, -0.889291524887085, 0.4498000741004944, -0.5633469820022583, 0.13599416613578796, -0.2541436553001404, 0.2252914160490036, -0.48970621824264526, -0.5826329588890076, -0.40661707520484924, -0.10053208470344543, -0.30520063638687134, 0.16595394909381866, -0.8297154903411865, 0.16431374847888947, -0.2715793251991272, 0.12113501876592636, -0.32863855361938477, -0.22250236570835114, -0.19103726744651794, 0.049770087003707886, -0.3302813172340393, -0.47413939237594604, 0.5320087671279907, 0.6054256558418274, -0.28283435106277466, 0.1440466046333313, -0.7777899503707886, -0.10759225487709045, 0.24956096708774567, -0.2291259765625, -0.636388897895813, -0.06299607455730438, 0.4830852746963501, 0.11099494993686676, 0.3375099301338196, 0.06685664504766464, -0.2430523782968521, -0.21525806188583374, -0.031722329556941986, 0.08691005408763885, 0.037763431668281555, -0.4535020589828491, 0.33758673071861267, -0.18532510101795197, -0.049702949821949005, -0.3013957738876343, -0.11952200531959534, -0.3700922429561615, 0.10399140417575836, 0.8089188933372498, 0.9452580809593201, 0.37469884753227234, 0.054309338331222534, -0.14676739275455475, 0.16924084722995758, 0.15184733271598816, 0.0774247795343399, 0.7995561361312866, 0.530451774597168, -0.6655863523483276, -0.2914552092552185, -0.6778668165206909, -0.23599886894226074, 0.20027318596839905, 0.09334200620651245, -0.1378994584083557, 0.5561148524284363, -0.196809321641922, 0.3829394578933716, 0.49890589714050293, -0.24201901257038116, 0.2273693084716797, -0.5183968544006348, 0.45981258153915405, -0.4141184687614441, 0.3535895347595215, 0.1807112693786621, 0.3427935838699341, 0.5133011341094971, -0.4165216088294983, 0.3213590979576111, -0.5978610515594482, 0.4613785743713379, 0.560720682144165, 0.5090963840484619, 0.6311003565788269, 0.23121678829193115, 0.11628862470388412, -0.33273759484291077, -0.177224263548851, 0.15706577897071838, -0.6861839294433594, -0.5526963472366333, 0.255706250667572, -0.13277719914913177, -0.27231359481811523, 0.6605607271194458, 0.2018541395664215, 0.4977748394012451, -0.6713729500770569, 0.6519744396209717, 0.5318616628646851, -0.20516441762447357, 0.6053006649017334, -0.3656926155090332, 0.2539781332015991, 0.8981878757476807, 0.3788139820098877, -0.8501629829406738, 0.1785694658756256, 0.5198051333427429, -0.5480238199234009, 0.2644190490245819, -0.07078710198402405, 0.22218966484069824, -0.20833197236061096, 0.3901178240776062, -0.1048101931810379, -0.36608773469924927, -0.0895484983921051, -0.3393039107322693, 0.3601818084716797, 0.005215140990912914, -0.288347065448761, 0.45513468980789185, 0.44133174419403076, 1.0850633382797241, 0.1731804609298706, 0.38551798462867737, -0.2892230451107025, -0.3610903024673462, 0.1500857174396515, -0.08960288763046265, -0.12240556627511978, 0.2070227712392807, -0.0006843442097306252, 0.15813858807086945, 0.6467630863189697, 0.8758465647697449, 0.4315328299999237, -0.20251868665218353, 0.4311901032924652, 0.2324148714542389, -0.29758501052856445, -0.22286877036094666, 1.0935094356536865, 1.0326918363571167, -0.09121489524841309, -0.12334853410720825, -0.899734616279602, -0.008113632909953594, -0.04170113429427147, -0.3042259216308594, 0.7099359631538391, 0.07975387573242188, -0.3164699077606201, 0.6650179624557495, -0.047845419496297836, 0.12992174923419952, -0.4213826656341553, -0.40274107456207275, -0.02017355151474476, -0.619572639465332, -0.12997783720493317, 0.7030460238456726, -0.02110193856060505, 0.5505643486976624, 0.16511142253875732, -0.06642095744609833, 0.04420320689678192, 0.4424881339073181, -0.4937058091163635, 0.00363810732960701, 0.3838767409324646, 0.12027884274721146, 0.11581659317016602, 0.7004110813140869, -0.3896287679672241, -0.5411379337310791, -0.5804761648178101, 0.11672869324684143, 0.18481500446796417, 0.27670907974243164, 0.27968740463256836, -0.1434202790260315, 0.2355780154466629, 0.7564059495925903, -0.021307148039340973, 0.30333688855171204, 0.06367196142673492, 0.19673265516757965, 0.39861175417900085, 0.47150254249572754, -0.4772234261035919, 0.3541640639305115, 0.39792394638061523, 0.7436557412147522, 0.391901433467865, -0.12884192168712616, 0.22132810950279236, -0.25725725293159485, -0.30884015560150146, 0.07991428673267365, -0.08212031424045563, -0.7805317640304565, 0.4652678370475769, 0.018447991460561752, 0.28021952509880066, 0.9466928839683533, 0.4625493288040161, 0.1267317682504654, 0.05058866739273071, 0.1308595985174179, -0.2709599733352661, 0.005448559299111366, 0.15894021093845367, -0.2687789499759674, -0.5352144241333008, -0.201009601354599, 0.37431222200393677, 0.4590497612953186, 0.25484710931777954, -0.4995056390762329, -0.2436884343624115, 0.8988380432128906, -0.2260536253452301, 0.7847926616668701, 0.2629193961620331, 0.3998262882232666, 0.08524103462696075, -0.23491474986076355, -0.47816503047943115, -0.5562109351158142, -0.82662433385849, -0.4257200360298157, -0.497784823179245, -0.332387775182724, -0.2360270470380783, 0.0067817894741892815, 0.6675674915313721, 0.1265043020248413, 0.6705468893051147, -0.21032056212425232, 0.11466816067695618, 0.12097960710525513, -0.534935474395752, 0.3965689539909363, 0.005966675002127886, 0.8664491176605225, -0.9786968231201172, -0.47224488854408264, 0.4938184320926666, 0.24274280667304993, -0.07739275693893433, 0.46190229058265686, 0.4268355667591095, -0.8623350858688354, 0.4705389142036438, 0.027571411803364754, -0.8862999081611633, 0.0062009962275624275, -0.03216668590903282, -0.5984154939651489, -0.024891074746847153, 0.799361526966095, 0.5921573638916016, 0.2946383059024811, -1.3260207176208496, 0.4925483763217926, 0.8189077377319336, -0.19666221737861633, 0.7031244039535522, -0.033009812235832214, -0.6142930388450623, -0.5379875898361206, 0.5550264120101929, 0.4075765013694763, -0.5665856599807739, 0.772786021232605, 1.028883457183838, 0.6031043529510498, 0.17430341243743896, -0.3524252772331238, -0.2100415825843811, -0.10879988223314285, -0.2934362292289734, 0.2522515058517456, -0.21700021624565125, 0.24142330884933472, -0.24131299555301666, -0.9420989751815796, 0.2256370484828949, -0.3865506947040558, -0.6019642353057861, -0.17736157774925232, -0.6815284490585327, 0.16188931465148926, 0.001941830851137638, 0.010227701626718044, 0.8238707780838013, -0.2814452052116394, 0.05991461127996445, 1.173779845237732, 0.4288172125816345, -0.2598363161087036, 0.22670362889766693, 0.28783804178237915, -0.4883084297180176, 0.13710224628448486, 0.46863308548927307, -0.21446548402309418, -0.5359169244766235, -0.07404749095439911, 0.17602097988128662, -0.05044649541378021, 0.16381579637527466, -0.43646135926246643, 1.0275676250457764, -0.17443574965000153, -0.8165767192840576, 0.7648144960403442, 0.18241742253303528, 0.6536787152290344, 0.5651447176933289, -0.034727923572063446, 0.6882747411727905, 0.1449345350265503, -0.8896126747131348, -1.342440128326416, 0.7052398920059204, 0.900814414024353, 0.18279628455638885, -0.5074880123138428, 0.3505708873271942, 0.06502320617437363, -0.33360379934310913, 1.0540807247161865, -0.4854966998100281, -0.1339922547340393, -0.10524342954158783, -0.18183323740959167, 0.387736976146698, -0.07993684709072113, 0.20276159048080444, -0.23093605041503906, -0.07592910528182983, 0.15898571908473969, -0.2382105141878128, 0.4777347445487976, 0.21308615803718567, -0.43627431988716125, 0.023080823943018913, 0.16806061565876007, 0.3813168406486511, -0.5052411556243896, 0.159539595246315, -0.3307521641254425, -0.751570463180542, -0.22825534641742706, 0.25025126338005066, 0.21223068237304688, 1.2307045459747314, -0.3846362233161926, 0.42466968297958374, 0.25788435339927673, -0.20771083235740662, 0.3309251070022583, -0.6801591515541077, 0.2159498631954193, -0.6894568204879761, 1.054730772972107, 0.6892892718315125, -0.7788728475570679, -0.39452487230300903, 0.5478250980377197, -0.36268067359924316, 0.13355393707752228, -0.42948397994041443, -0.48517102003097534, -0.9154101610183716, 0.04204939305782318, -0.553311824798584, 0.08959424495697021, -0.49680131673812866, -0.17458635568618774, -0.21753567457199097, -0.02000749111175537, 0.6780630946159363, -1.4734102487564087, 0.10999023914337158, -0.7928770184516907, 0.20373916625976562, 0.5023478269577026, -0.2300526201725006, -0.050342999398708344, -0.2855615019798279, -0.7935279607772827, 0.2180074155330658, -0.19960814714431763, 0.23567354679107666, -0.05572812259197235, 0.542668342590332, -0.6241847276687622, 0.0605914369225502, 0.02342197112739086, -0.18403702974319458, 0.13732993602752686, 0.4343581199645996, 0.13855940103530884, -0.10683291405439377, 1.2234612703323364, -0.010500159114599228, -0.18487994372844696, 0.07320737838745117, -0.634064257144928, -0.1654118299484253, -0.22323355078697205, -0.5824495553970337, -0.24450600147247314, -0.12127985805273056, 0.05810878798365593, 0.9666794538497925, -0.011993907392024994, 0.1514672338962555, 0.10036752372980118, -0.3355664610862732, -0.34312790632247925, -0.6920209527015686, 0.45061948895454407, -0.41277649998664856, -0.5883772373199463, 0.530026912689209, 0.2837327718734741, -0.34418442845344543, -0.15172463655471802, 0.5904768705368042, -0.23383232951164246, 0.5087993144989014, -0.7090471982955933, 0.23149846494197845, -0.16887767612934113, -1.1402584314346313, 0.5730478167533875, 0.012041497975587845, 0.20734351873397827, -0.5494070053100586, 0.24944953620433807, -0.9883666634559631, -0.8232433199882507, -0.1517201066017151, 0.22528833150863647, -0.09079928696155548, 0.24826550483703613, 0.0011168878991156816, 0.32818329334259033, -0.4083581566810608, -0.7219271659851074, 0.07940171658992767, 0.7467790842056274, -0.07823716849088669, 0.2683059573173523, 0.29904019832611084, -0.3151964545249939, 0.11871453374624252, 0.18436002731323242, -0.8537328243255615, 0.5624972581863403, -0.06432802975177765, 0.038722727447748184, -0.7225075364112854, -0.3475774824619293, -0.1841156780719757, 0.2645875811576843, -0.17102622985839844, 0.5997183322906494, -0.3459625840187073, 0.2757951021194458, -0.14353638887405396, 0.2658874988555908, 0.06771066039800644, -0.40728867053985596, -0.27207323908805847, 0.9136993288993835, 0.34655433893203735, -0.9968461394309998, -0.6374425888061523, 1.019163727760315, -0.44288015365600586, 0.40159741044044495, -0.8048667311668396, -0.05708351731300354, -0.47374141216278076, -0.475238174200058, -0.4429175853729248, -1.0678589344024658, -0.48624366521835327, -0.37767165899276733, -1.072183609008789, -0.146659255027771, 0.30148181319236755, 0.1830885112285614, -0.35716792941093445, 0.09332846105098724, -1.1771589517593384, -0.5767343640327454, -0.05196281149983406, -0.8452861905097961, -0.820817232131958, 0.10660655796527863, -0.3275367021560669, 0.4343809187412262, -0.12672139704227448, -0.07833826541900635, 0.14118406176567078, 0.11575998365879059, 0.6428738236427307, 0.1567842662334442, -0.4411808252334595, -0.8346196413040161, -0.01835569739341736, 0.3020392060279846, -0.4527197480201721, -0.18426242470741272, -0.6343744993209839, 0.44760575890541077, -0.962486743927002, -0.23686476051807404, 0.018686963245272636, -0.5357061624526978, 0.1074446588754654, -0.18702968955039978, 0.5251051783561707, -0.36052054166793823, 0.36638733744621277, -0.07192812860012054, 0.226655513048172, 1.1671273708343506, 0.6594514846801758, -0.3814956247806549, 0.03966563194990158, -0.37358999252319336, -0.7333441972732544, -0.1617760956287384, -0.6052759885787964, 0.10735057294368744, -0.7398834228515625, -0.08321373164653778, 0.43955081701278687, 0.6847802400588989, 0.5351370573043823, 0.7212200164794922, -0.2893331050872803, 0.09653725475072861, -0.8926154375076294, -0.2666952908039093, 0.8209788203239441, 0.058274462819099426, 0.3372785449028015, -0.769546627998352, -0.6287184357643127, 0.9424471855163574, -0.41476401686668396, -0.6120685935020447, -0.6195699572563171, 0.3405819833278656, 0.7036606073379517, -0.6593998670578003, 0.8454155921936035, -0.4630768895149231, -0.9390077590942383, -0.36389023065567017, 0.7827029228210449, -0.08745956420898438, 0.43901628255844116, 0.025453772395849228, -0.8664678335189819, -0.7974492311477661, -0.6821454763412476, 0.5165139436721802, -0.1023799479007721, -0.7064367532730103, 0.37665796279907227, 0.056959688663482666, -0.4753555655479431, 0.22306016087532043, 0.18239010870456696, -0.6674978733062744, -0.7571128606796265, -0.10305634140968323, -0.19936013221740723, 0.0268014594912529, -0.8253340721130371, 0.3217523396015167, -0.19405058026313782, -0.4925744831562042, 0.25807133316993713, 0.1344379335641861, 0.009768517687916756, 0.6803734302520752, -0.43935081362724304, 0.8866972327232361, -0.5366173982620239, 0.24866314232349396, 0.22702914476394653, -0.5755760073661804, -0.41615867614746094, -0.5627160668373108, -0.47265228629112244, 0.05954756960272789, -0.24182984232902527, 0.8574879169464111, -0.0015793065540492535, -0.08847297728061676, 0.16312748193740845, -0.21078366041183472, 0.9478899836540222, 0.25296640396118164, -0.330390602350235, 0.0336809903383255, -1.178037405014038, -0.46410903334617615, -0.5864179134368896, 0.2426590621471405, -0.24575041234493256, 0.2039690464735031, 0.32594799995422363, -0.2958981692790985, 0.27827537059783936, 0.763681173324585, -0.11154249310493469, -0.39814943075180054, -0.7775817513465881, -0.5881688594818115, -0.17592120170593262, -0.5116637945175171, -1.2837778329849243, 0.2854938805103302, 0.1322559118270874, 0.10984107851982117, -0.8653427362442017, 0.028981907293200493, 0.4317880868911743, -0.95765221118927, 0.46338337659835815, -0.34261468052864075, -0.528760552406311, -0.314754843711853, -0.7503359913825989, -0.07716812193393707, -0.5646520256996155, -0.5195375680923462, 0.7715235948562622, -0.25944212079048157, -0.25752294063568115, -0.33110111951828003, 1.139336347579956, -0.17642492055892944, 0.4716278314590454, 0.5433546900749207, -0.6762627959251404, -0.26542234420776367, 0.45308083295822144, -0.585403323173523, -0.28790029883384705, 0.30441349744796753, -0.5395258665084839, 0.18222147226333618, -0.21219147741794586, -0.6493474841117859, 0.08100977540016174, 0.6810209155082703, 0.33755946159362793, -0.16273123025894165, -0.6769039034843445, 0.1776217520236969, 0.16222134232521057, -0.22858980298042297, 0.17173320055007935, -0.4865913391113281, 0.04796755313873291, -0.03250329941511154, -0.07271918654441833, -0.09903872758150101, 0.9634559154510498, 0.06627896428108215, 0.714301586151123, -0.11389243602752686, -0.15169981122016907, -0.37199583649635315, -0.4560089111328125, 0.4874403476715088, 0.7235758900642395, 0.7662373781204224, -0.29374727606773376, -0.2618263363838196, -0.4647192358970642, -0.5828865766525269, -0.20005011558532715, 0.17266425490379333, -0.6772943735122681, -0.35168376564979553, 0.0876564234495163, 0.15590648353099823, 0.7666698098182678, 0.5891786217689514, -0.10724254697561264, -0.2175522893667221, 0.008396893739700317, -1.0879580974578857, 0.5576242208480835, -0.7548943161964417, -0.7266000509262085, 0.43383198976516724, -0.04565362632274628, 0.33923399448394775, 0.5853590965270996, -0.5588745474815369, 1.009162187576294, 0.05582570657134056, 0.1683155596256256, -0.007489935029298067, 0.659776508808136, -0.40543556213378906, 0.4134381413459778, -0.12951865792274475, -0.10483626276254654, 0.5051218271255493, 0.7457414865493774, -0.2870978116989136, -0.7547118067741394, -0.1924174427986145, 0.5623708963394165, 0.0762634128332138, -0.12630745768547058, 0.23503664135932922, 0.0586877316236496, 0.3130100965499878, 0.048482343554496765, -0.05417347326874733, 0.2046372890472412, 0.032894957810640335, 0.4890325665473938, -0.41859564185142517, -0.26895672082901, -0.5738447904586792, 0.58393394947052, 0.3109152019023895, -0.18065820634365082, 0.027993248775601387, 0.8967103958129883, 0.4539809226989746, -0.12366233766078949, 0.4015502333641052, 0.15992972254753113, 1.0180461406707764, -0.6093931198120117, 0.1992175579071045, 0.19568124413490295, 0.7248982191085815, -0.028930887579917908, 0.2752005159854889, -0.15619899332523346, 0.3702312111854553, 0.3951161205768585, 0.43926793336868286, 0.7685842514038086, 0.32741934061050415, 1.1264684200286865, 0.12230154871940613, 0.12786346673965454, -0.05078570544719696, -0.19103555381298065, 0.09462840110063553, 0.1452247053384781, 0.005909613333642483, 0.06134875863790512, -0.3774837553501129, 0.3825102746486664, -0.4120026230812073, 0.6677396297454834, -0.4635433554649353, -0.13550591468811035, -0.2552849352359772, -0.06037662923336029, 0.11864941567182541, 0.04863390326499939, -0.0331512987613678, 0.9711631536483765, 0.23478248715400696, -0.02221795916557312, -0.3678882122039795, 1.4082235097885132, -0.6345883011817932, 0.2599914073944092, -0.0003701271489262581, -0.015485215932130814, 0.3832433819770813, 0.43068718910217285, -0.11989326775074005, -0.7496286630630493, -0.30634790658950806, 0.1502809077501297, -0.5029042959213257, -0.16064248979091644, 0.5062052011489868, -0.7151878476142883, -0.4093012809753418, -1.10410475730896, 0.39750969409942627, -0.5461227893829346, 0.12443769723176956, 0.4068991541862488, -0.7789945602416992, -0.9484771490097046, 0.7209516167640686, -0.22648775577545166, -0.16272541880607605, 0.6603052020072937, 0.972147524356842, 0.5493028163909912, 0.7583420276641846, 0.30721449851989746, -0.6630048751831055, -0.5294212102890015, 0.05764961242675781, -0.17384858429431915, -0.34869256615638733, -0.014475926756858826, -0.25822556018829346, -0.22984355688095093, -0.7495354413986206, -0.033489592373371124, 0.0808631107211113, 0.22956083714962006, -0.2096136063337326, -0.6006050109863281, -0.833901047706604, 0.6830310821533203, -0.5292360782623291, -0.36011064052581787, 0.6168030500411987, -0.09093892574310303, 0.5380819439888, 0.05302916467189789, -0.6187105178833008, -0.22121575474739075, -0.3444387912750244, -0.18964996933937073, 0.0799502283334732, -0.44388413429260254, -0.22860372066497803, -0.35321754217147827, -0.3168877959251404, -0.33625316619873047, -0.18058758974075317, -1.0457096099853516, -0.18343675136566162, 0.4016413688659668, 0.516052782535553, -0.05900784209370613, 0.9703093767166138, 0.49091529846191406, -0.15118955075740814, 0.9117608070373535, 0.4066801071166992, -0.30931276082992554, 0.1142701506614685, 0.7553831338882446, 0.241580069065094, 0.34992051124572754, -0.1295587420463562, 0.27035748958587646, -0.02961288020014763, -0.21095183491706848, -0.39714980125427246, 0.330476850271225, -0.4364750385284424, -0.7579472661018372, 0.09294450283050537, -0.25669148564338684, -0.10571806132793427, -0.1885932981967926, -0.8688524961471558, 0.0226516742259264, -0.8422892689704895, 0.20358097553253174, -0.19324621558189392, -0.47105252742767334, 0.8990613222122192, 0.020780909806489944, -0.50469970703125, -1.197982907295227, 4.086843013763428, 1.0397038459777832, -0.0978161096572876, 0.6380754709243774, 0.6330581903457642, 0.9058765172958374, 0.586556077003479, -0.5802891850471497, -0.043905504047870636, -0.32248467206954956, 0.24783442914485931, 0.29808515310287476, 0.41475340723991394, -0.3690061569213867, 0.610692024230957, 0.5876086950302124, 0.1192089319229126, 0.2847370207309723, 0.02988608554005623, -0.9095126390457153, -1.05558180809021, 0.5561990737915039, 0.1170007586479187, -0.10836025327444077, 0.05610932782292366, 0.11284532397985458, 0.12342163175344467, -0.43722060322761536, -0.5785935521125793, -0.5154092311859131, -0.7210123538970947, -0.34137213230133057, 0.39425450563430786, 0.22802212834358215, -0.4004197120666504, 1.1376087665557861, -0.01830730028450489, -0.803054690361023, -0.23650717735290527, 0.7302759885787964, -0.3464248776435852, -0.4777071177959442, 0.8702357411384583, -0.2991630434989929, 0.11179091036319733, 0.46597808599472046, -0.32822662591934204, -0.32687491178512573, 0.10809213668107986, -0.4302998185157776, 0.3376367688179016, -0.17230217158794403, 0.08814184367656708, 0.5492735505104065, -0.6884812116622925, 0.06374259293079376, -0.3208545446395874, -0.2498408854007721, 0.7300695180892944, 0.6283119916915894, 0.16403239965438843, -0.10837454348802567, -0.4164945185184479, -0.10496863722801208, -1.1003607511520386, 0.1796211302280426, -0.022890549153089523, 0.7945359945297241, -0.10915327817201614, -0.5564196705818176, -0.009155014529824257, 0.036558546125888824, -0.8604947924613953, 0.3874795436859131, 1.1715545654296875, 0.484572172164917, -0.5333455801010132, -0.02388659119606018, 0.14371632039546967, -0.1127525120973587, -0.2894929349422455, -0.7862710952758789, -0.19220557808876038, 0.31854867935180664, 0.45581167936325073, 0.719414234161377, -0.4495258331298828, -0.30622565746307373, 0.5750972032546997, 0.9104995727539062, 0.5571521520614624, 0.8690816164016724, 0.20888590812683105, -0.29034551978111267, 0.15037187933921814]}\n"
     ]
    }
   ],
   "source": [
    "# Fetch sample document to verify if the collection retrieves data\n",
    "sample = collection.find_one({'description_embedding': {'$exists': True}})\n",
    "print(\"Sample document:\", sample)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "engine_H",
   "language": "python",
   "name": "engine_h"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
